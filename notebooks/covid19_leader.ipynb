{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "install conda env, recommendation is into project dir\n",
    "\n",
    "`conda install -p ./.venv -c conda-forge -c bioconda --file ./notebooks/covid19_leader.requirements.txt`\n",
    "\n",
    "`. activate ./.venv`\n",
    "\n",
    "Add associate input and output folders. The default location is `./input` and `./output`\n",
    "\n",
    "NOTE: for `./sars_cov2_leader.sh` to work on mac you'll also need to install md5sum (e.g. brew install md5sha1sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor # For parallel processing\n",
    "import subprocess\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "INPUT_DIR = os.getenv('INPUT_DIR', \"./input\")\n",
    "SARS_COV_2_LEADER_PROGRAM_PATH = os.getenv('SARS_COV_2_LEADER_PROGRAM_PATH', \"./scripts/sars_cov2_leader.sh\")\n",
    "OUTPUT_DIR = os.getenv('OUT_DIR', \"./output\")\n",
    "THREADS = os.getenv('THREADS', 12)\n",
    "EXECUTE_COMMANDS = os.getenv('EXECUTE_COMMANDS', True)\n",
    "\n",
    "PROJECT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bam_files(input_dir:Path, bam_extension:str=\".bam\") -> list[Path]:\n",
    "    bam_files:list[Path] = []\n",
    "    for here, dirs, files in os.walk(input_dir, topdown=True):\n",
    "        for file in files:\n",
    "            if file.endswith(bam_extension):\n",
    "                bam_files.append(os.path.abspath(os.path.join(here, file)))\n",
    "    return bam_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_files = get_bam_files(INPUT_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of bam_files: {len(bam_files)}\\nFirst 10: {bam_files[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If this command fails it creates a bunch of folders in the main directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leaders_in_bam(bam_files: list[Path], output:Path, program_path:Path=SARS_COV_2_LEADER_PROGRAM_PATH, reference_name:str='MN908947.3', quality:int=30) -> list[Path]:\n",
    "    output_folders: list[Path] = []\n",
    "    commands: list[str] = []\n",
    "    process_threads: int = 1 # Set to 1 because each process runs really fast so figure its more optimal to run 1 per thread then pooling more threads to 1 \n",
    "    try:    \n",
    "        for bam_file in bam_files:\n",
    "            expected_output_folder:Path = os.path.join(output, f\"{os.path.basename(bam_file)[:-4]}_leader_data\")\n",
    "            out_leaders_txt:Path = os.path.join(output, os.path.basename(bam_file))[:-4] + \".leaders.txt\"\n",
    "            if not os.path.isdir(expected_output_folder):    \n",
    "                command:str = f\"\" \\\n",
    "                    f\"{program_path} -i {bam_file} -r {reference_name} -q {quality} -t {process_threads} -o {out_leaders_txt};\" \\\n",
    "                    f\"mv {os.path.basename(bam_file)[:-4]}_leader_data {expected_output_folder};\"\n",
    "                commands.append(command)\n",
    "        with ThreadPoolExecutor(max_workers=math.floor(THREADS/process_threads)) as executor:\n",
    "            for i, command in enumerate(commands):\n",
    "                if EXECUTE_COMMANDS:\n",
    "                    executor.submit(subprocess.run, command, shell=True)\n",
    "                    print(f\"{i}/{len(commands)} Ran command: {command}\", end=\"\\r\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Command: {command}\")\n",
    "        print(f\"Error processing {bam_file}\")\n",
    "    return output_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leader_folder = find_leaders_in_bam(bam_files, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bam_files = get_bam_files(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_depth_on_leader_data(bam_files:list[Path], output:Path) -> list[Path]:\n",
    "    calculated_depth_files:list[Path] = []\n",
    "    commands:str = []\n",
    "    try:\n",
    "        for bam_file in bam_files:\n",
    "            output_depth_file:Path = os.path.join(output, os.path.basename(bam_file))[:-4] + \".depth.txt\"\n",
    "            calculated_depth_files.append(output_depth_file)\n",
    "            if not os.path.isfile(output_depth_file):\n",
    "                command:str = f\"\" \\\n",
    "                    f\"samtools depth {bam_file} > {output_depth_file}; \" \n",
    "                commands.append(command)\n",
    "        with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "            for i, command in enumerate(commands):\n",
    "                print(f\"\\n{command}\")\n",
    "                if EXECUTE_COMMANDS:\n",
    "                    executor.submit(subprocess.run, command, shell=True)\n",
    "                    print(f\"{i}/{len(commands)}\\ Depth calculated: {bam_file}\", end=\"\\r\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Command: {command}\")\n",
    "        print(f\"Error processing {bam_file}\")\n",
    "    return calculated_depth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_files = calculate_depth_on_leader_data(output_bam_files, OUTPUT_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function if you already have depth files made\n",
    "def scan_for_depth_files(output:Path) -> list[Path]:\n",
    "    depth_files:list[Path] = []\n",
    "    for root, dirs, files in os.walk(output):\n",
    "        for file in files:\n",
    "            if file.endswith(\".depth.txt\"):\n",
    "                depth_files.append(os.path.abspath(os.path.join(root, file)))\n",
    "    return depth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_files = scan_for_depth_files(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depth_on_sites_of_interest(depth_files: list[Path], sites_of_interest: list[int], output: Path):\n",
    "    with open(output, \"w+\") as out_file:\n",
    "        for depth_file in depth_files:\n",
    "            sample_name:str = os.path.basename(depth_file).split(\".\")[0]\n",
    "            with open(depth_file, \"r\") as depth_file_stream:\n",
    "                for line in depth_file_stream:\n",
    "                    position = int(line.strip().split(\"\\t\")[1])\n",
    "                    depth = int(line.strip().split(\"\\t\")[2])\n",
    "                    if position in sites_of_interest:\n",
    "                        print(f\"{position}\\t{depth}\\t{sample_name}\", file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_from_paper = [55, 21552, 25385, 26237, 26469, 27041, 27388, 27644, 27884, 28256, 29530]\n",
    "parse_depth_on_sites_of_interest(depth_files, sites_from_paper, f\"{OUTPUT_DIR}/COVID_leader_splice_sites.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the proportions are based on the sites included, beware of missing sites and double counting sites (nearby sites which are artificats)\n",
    "def make_proportional_samples(parsed_depth_file:Path, proportional_file:Path):\n",
    "    sample_count:dict = {}\n",
    "    with open(parsed_depth_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line:str = line.strip()\n",
    "            if not line.startswith(\"#\"):    \n",
    "                line = line.split(\"\\t\")\n",
    "                try:\n",
    "                    pos:int = int(line[0])\n",
    "                    counts:int = int(line[1])\n",
    "                    sample:str = line[2]\n",
    "                    if sample not in sample_count:\n",
    "                        sample_count[sample] = {}\n",
    "                    sample_count[sample][pos] = counts\n",
    "                except Exception:\n",
    "                    print(line)\n",
    "    with open(proportional_file, \"w+\") as out_file:\n",
    "        print(f\"#sample_name\\tposition\\tproportion\\tcount\", file=out_file)\n",
    "        for key in sample_count:\n",
    "            total:int = 0\n",
    "            total:int = sum(sample_count[key].values())\n",
    "            for pos in sample_count[key]:\n",
    "                print(f\"{key}\\t{pos}\\t{sample_count[key][pos]/total}\\t{sample_count[key][pos]}\", file=out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_proportional_samples(f\"{OUTPUT_DIR}/COVID_leader_splice_sites.tsv\", f\"{OUTPUT_DIR}/COVID_leader_splice_sites.proportional.tsv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e29cf017519e0256b09e396015674c9d1006442c598bfabae054b082acb2a55"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
