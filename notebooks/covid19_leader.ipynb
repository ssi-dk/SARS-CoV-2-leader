{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of subgenomic RNA sites for COVID-19\n",
    "This project is based on [paper](https://www.nature.com/articles/s41467-020-19883-7#Sec12) which looks at open reading frames (ORF) and their potential role in COVID-19. The code here is a fork of the original [repo](https://github.com/achamings/SARS-CoV-2-leader) which filters whole genome sequence (WGS) samples on the sequence motif of GTAGATCTGTTCTCT. The resulting mapped sequences are then used to detect subgenomic RNA sites through the relative depth of coverage at each site of interest as defined in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation/Setup\n",
    "Please ensure you've followed instructions in `./README.md` to install the conda environment which this is based on. It contains the required libraries for also running this jupyter notebook. If you follow the default set up then ensure your Jupyter server is pointing to `./venv/bin/python`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Import of libraries. These are used for data processing and type hinting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor # For parallel processing\n",
    "import subprocess\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check to ensure the currect directory is the root of the project. This should be the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "A `./.env` file in the project root can be used to store variables. This is .gitignored so paths are not commited to the repo. Please see `./notebooks/covid19_leader.env.template` for an example. This file also can be skipped and then the default values will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "INPUT_DIR = os.getenv('INPUT_DIR', \"./input\")\n",
    "SARS_COV_2_LEADER_PROGRAM_PATH = os.getenv('SARS_COV_2_LEADER_PROGRAM_PATH', \"./scripts/sars_cov2_leader.sh\")\n",
    "OUTPUT_DIR = os.getenv('OUT_DIR', \"./output\")\n",
    "THREADS = os.getenv('THREADS', 12)\n",
    "EXECUTE_COMMANDS = os.getenv('EXECUTE_COMMANDS', True)\n",
    "\n",
    "PROJECT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and calls\n",
    "The notebook is organized into a defined function for utility then a call associated with it. Additional informational code may be also included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bam files\n",
    "Gathers all bam files in the input directory and returns a list of the full file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bam_files(input_dir:Path, bam_extension:str=\".bam\") -> list[Path]:\n",
    "    bam_files:list[Path] = []\n",
    "    for here, dirs, files in os.walk(input_dir, topdown=True):\n",
    "        for file in files:\n",
    "            if file.endswith(bam_extension):\n",
    "                bam_files.append(os.path.abspath(os.path.join(here, file)))\n",
    "    return bam_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_files = get_bam_files(INPUT_DIR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check on expected inputs and peek at first few results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of bam_files: {len(bam_files)}\\nFirst 10: {bam_files[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find leaders in bam\n",
    "This wrapper runs the original script `./scripts/sars_cov2_leader.sh` against each bam file found. This will filter the mapped reads for reads with the leader sequence which will be accessed later.\n",
    "\n",
    "As the original script was not designed to run in batch mode this handles that and requires that the output file is unique to each sample as their is no lock on writing to the output file. The script also assumes that it operates in the current directory so will place its output in `./`. This is not ideal as if the script fails temporary files will be left behind, however, the decision was made to not adjust the original code in case of updates to the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leaders_in_bam(bam_files: list[Path], output:Path, program_path:Path=SARS_COV_2_LEADER_PROGRAM_PATH, reference_name:str='MN908947.3', quality:int=30) -> list[Path]:\n",
    "    output_folders: list[Path] = []\n",
    "    commands: list[str] = []\n",
    "    process_threads: int = 1 # Set to 1 because each process runs really fast so figure its more optimal to run 1 per thread then pooling more threads to 1 \n",
    "    try:    \n",
    "        for bam_file in bam_files:\n",
    "            expected_output_folder:Path = os.path.join(output, f\"{os.path.basename(bam_file)[:-4]}_leader_data\")\n",
    "            out_leaders_txt:Path = os.path.join(output, os.path.basename(bam_file))[:-4] + \".leaders.txt\"\n",
    "            if not os.path.isdir(expected_output_folder):    \n",
    "                command:str = f\"\" \\\n",
    "                    f\"{program_path} -i {bam_file} -r {reference_name} -q {quality} -t {process_threads} -o {out_leaders_txt};\" \\\n",
    "                    f\"mv {os.path.basename(bam_file)[:-4]}_leader_data {expected_output_folder};\" # Dislike this as if it fails it leaves temp files everywhere. I'm avoiding modifying the original code though so leaving this as is\n",
    "                commands.append(command)\n",
    "        with ThreadPoolExecutor(max_workers=math.floor(THREADS/process_threads)) as executor:\n",
    "            for i, command in enumerate(commands):\n",
    "                if EXECUTE_COMMANDS:\n",
    "                    executor.submit(subprocess.run, command, shell=True)\n",
    "                    print(f\"{i}/{len(commands)} Ran command: {command}\", end=\"\\r\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\"\\\n",
    "            f\"Command: {command if 'command' in locals() else None}\\n\"\\\n",
    "            f\"Error processing: {bam_file if 'bam_file' in locals() else None}\")\n",
    "    return output_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leader_folder = find_leaders_in_bam(bam_files, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanning for bam files for output instead of using `leader_folder` parsing in case user starts with previous output or adds completed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bam_files = get_bam_files(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate depth on the leader data\n",
    "This runs the `samtools depth` command on all the filtered bam files so that the coverage can be extracted for sites of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_depth_on_leader_data(bam_files:list[Path], output:Path) -> list[Path]:\n",
    "    calculated_depth_files:list[Path] = []\n",
    "    commands:str = []\n",
    "    try:\n",
    "        for bam_file in bam_files:\n",
    "            output_depth_file:Path = os.path.join(output, os.path.basename(bam_file))[:-4] + \".depth.txt\"\n",
    "            calculated_depth_files.append(output_depth_file)\n",
    "            if not os.path.isfile(output_depth_file):\n",
    "                command:str = f\"\" \\\n",
    "                    f\"samtools depth {bam_file} > {output_depth_file}; \" \n",
    "                commands.append(command)\n",
    "        with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "            for i, command in enumerate(commands):\n",
    "                print(f\"\\n{command}\")\n",
    "                if EXECUTE_COMMANDS:\n",
    "                    executor.submit(subprocess.run, command, shell=True)\n",
    "                    print(f\"{i}/{len(commands)}\\ Depth calculated: {bam_file}\", end=\"\\r\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\"\\\n",
    "            f\"Command: {command if 'command' in locals() else None}\\n\"\\\n",
    "            f\"Error processing: {bam_file if 'bam_file' in locals() else None}\")\n",
    "    return calculated_depth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_files = calculate_depth_on_leader_data(output_bam_files, OUTPUT_DIR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan for depth files\n",
    "Gathers the depth files, is equivalent to running `calculate_depth_on_leader_data` but allows for inclusion of data from previous runs or if you lack inputs to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_for_depth_files(output:Path) -> list[Path]:\n",
    "    depth_files:list[Path] = []\n",
    "    for root, dirs, files in os.walk(output):\n",
    "        for file in files:\n",
    "            if file.endswith(\".depth.txt\"):\n",
    "                depth_files.append(os.path.abspath(os.path.join(root, file)))\n",
    "    return depth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_files = scan_for_depth_files(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sites of interest\n",
    "The sites of interest from the [paper](https://www.nature.com/articles/s41467-020-19883-7#Sec12). An attempt was also done to detect peeks in the depth data by considering previous position, minimum depth, and change from previous position. This did lead to comparable sites of interest but also included spurious sites of interest, for example positions within the read length of another position due to deletions or that the change from previous position was too small or small mismappings which skewed positions by a handful of positions. A generic solution would of course be ideal but this is a project for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_from_paper = [55, 21552, 25385, 26237, 26469, 27041, 27388, 27644, 27884, 28256, 29530]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse depth on sites of interest\n",
    "Retrieve the coverage of positions in the sites of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depth_on_sites_of_interest(depth_files: list[Path], sites_of_interest: list[int], output: Path):\n",
    "    with open(output, \"w+\") as out_file:\n",
    "        for depth_file in depth_files:\n",
    "            sample_name:str = os.path.basename(depth_file).split(\".\")[0]\n",
    "            with open(depth_file, \"r\") as depth_file_stream:\n",
    "                print(f\"#sample_name\\tposition\\tcount\", file=out_file)\n",
    "                for line in depth_file_stream:\n",
    "                    position = int(line.strip().split(\"\\t\")[1])\n",
    "                    depth = int(line.strip().split(\"\\t\")[2])\n",
    "                    if position in sites_of_interest:\n",
    "                        print(f\"{sample_name}\\t{position}\\t{depth}\", file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_depth_on_sites_of_interest(depth_files, sites_from_paper, f\"{OUTPUT_DIR}/COVID_leader_splice_sites.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make proportional samples\n",
    "Calculates the proportion on sites of interest for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_proportional_samples(parsed_depth_file:Path, proportional_file:Path):\n",
    "    sample_count:dict = {}\n",
    "    with open(parsed_depth_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line:str = line.strip()\n",
    "            if not line.startswith(\"#\"):    \n",
    "                line = line.split(\"\\t\")\n",
    "                try:\n",
    "                    sample:str = line[0]\n",
    "                    pos:int = int(line[1])\n",
    "                    counts:int = int(line[2])\n",
    "                    if sample not in sample_count:\n",
    "                        sample_count[sample] = {}\n",
    "                    sample_count[sample][pos] = counts\n",
    "                except Exception:\n",
    "                    print(line)\n",
    "    with open(proportional_file, \"w+\") as out_file:\n",
    "        print(f\"#sample_name\\tposition\\tproportion\\tcount\", file=out_file)\n",
    "        for key in sample_count:\n",
    "            total:int = 0\n",
    "            total:int = sum(sample_count[key].values())\n",
    "            for pos in sample_count[key]:\n",
    "                print(f\"{key}\\t{pos}\\t{sample_count[key][pos]/total}\\t{sample_count[key][pos]}\", file=out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_proportional_samples(f\"{OUTPUT_DIR}/COVID_leader_splice_sites.tsv\", f\"{OUTPUT_DIR}/COVID_leader_splice_sites.proportional.tsv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e29cf017519e0256b09e396015674c9d1006442c598bfabae054b082acb2a55"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
